# pytorch_lightning==2.1.3
seed_everything: true
trainer:
  precision: 16-mixed
  logger:
    # WandbLogger(name=RUN_NAME, project=PROJECT_NAME)
    - class_path: pytorch_lightning.loggers.wandb.WandbLogger
      init_args:
        name: initial try
        project: finetune_lightglue
        dir: "/home/user/Documents/projects/Metaworld/keyframes/finetune_lightglue/logs"
        save_dir: "/home/user/Documents/projects/Metaworld/keyframes/finetune_lightglue/logs"
  callbacks:
    # ModelCheckpoint(save_weights_only=True, mode="min", monitor="val_loss"),
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_weights_only: true
        mode: min
        monitor: val_loss
    # LearningRateMonitor("step"),
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  fast_dev_run: false
  max_epochs: 1000
  # on single RTX3090, log_every_n_steps should be around 50 / accumulate_grad_batches
  log_every_n_steps: 20
  enable_progress_bar: True
  accumulate_grad_batches: 3
  gradient_clip_val: 0.5
  # default_root_dir: /home/user/Documents/projects/Metaworld/keyframes/finetune_lightglue/logs
model:
  n_layers: 9
  image_size: 
    - 360
    - 480

  # adamw optimizer
  optimizer_lr: 1.3e-4  # default of nanoGPT is 6e-4, minimum learning rate is 6e-5
  # milestones: [1000,1500,20000]
  # gamma: 0.1
  optimizer_warmup: 250  # default of nanoGPT is 2000
  optimizer_max_iters: 1200000  # default of nanoGPT is 600'000
  optimizer_start_iter: 0
data:
  data_folder: /media/user/ssd2t/datasets2/metaworld_keyframes/all_envs
  train_env_ids: [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44]
  val_env_ids: [45, 46, 47, 48, 49]
  data_folder_predict: null
  batch_size: 12
  num_workers: 6
  num_samples_per_env: 200
  max_keypoints: 10
  eps: 0.01
ckpt_path: null
