{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Optimize focal length for each image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Open3D INFO] Resetting default logger to print to terminal.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Normal estimation\"\"\"\n",
    "ROOT_DIR = \"/home/user/Documents/projects/Metaworld\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "%matplotlib widget  \n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms import PILToTensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import importlib\n",
    "from typing import List\n",
    "from easydict import EasyDict as edict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from lightglue import viz2d\n",
    "from einops import rearrange\n",
    "\n",
    "import open3d as o3d\n",
    "from open3d.web_visualizer import draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyframes.align_ft_spair import dataset\n",
    "from keyframes.align_ft_spair.utils import geo_utils, spair_utils, ft_align_utils, depth_match_utils, geom_utils, img_utils, kpt_likelihood_utils, kpt_likelihood_opt_v2, focal_opt_utils, peak_extraction_utils\n",
    "from keyframes.align_ft_spair import pl_modules\n",
    "from keyframes.align_ft_spair.ext import projection_network, utils_correspondence, utils_dataset\n",
    "from utils import torch_utils, plotly_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "spair_data_folder=\"/media/user/ssd2t/datasets2/SPair-71k\"\n",
    "embds_folder_geo=\"/media/user/EXTREMESSD/datasets/SPair-71k/geo\"\n",
    "embds_folder_sd=\"/media/user/EXTREMESSD/datasets/SPair-71k/DiffusionFeatures60x60\"\n",
    "img_files_np_path_train=\"/home/user/Documents/projects/diffusion-features/experiments/ft_align/aeroplane_train_files_unique.npy\"\n",
    "img_files_np_path_eval=\"/home/user/Documents/projects/diffusion-features/experiments/ft_align/aeroplane_eval_files_unique.npy\"\n",
    "# depth_folder = \"/media/user/EXTREMESSD/datasets/SPair-71k/miragold\"\n",
    "depth_folder = \"/media/user/EXTREMESSD/datasets/SPair-71k/DepthAnythingV2\"\n",
    "masks_folder = \"/media/user/EXTREMESSD/datasets/SPair-71k/SegAnyMasks\"\n",
    "\n",
    "# kpt_indices=[3, 4, 5]\n",
    "category=\"aeroplane\"\n",
    "img_size=960\n",
    "embd_size=60\n",
    "pad=True\n",
    "\n",
    "img_files_train = np.load(img_files_np_path_train).tolist()\n",
    "img_files_eval = np.load(img_files_np_path_eval).tolist()\n",
    "\n",
    "flips_train = [False]\n",
    "flips_eval = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "img_embds_train, img_embds_hat_train = geo_utils.load_geo_embds(\n",
    "    img_files_train,\n",
    "    embds_folder_dino=embds_folder_geo,\n",
    "    embds_folder_sd=embds_folder_sd,\n",
    "    flips=flips_train,\n",
    ")\n",
    "img_embds_train = img_embds_train.detach().cpu()\n",
    "img_embds_hat_train = img_embds_hat_train.detach().cpu()\n",
    "\n",
    "# load embeddings\n",
    "img_embds_eval, img_embds_hat_eval = geo_utils.load_geo_embds(\n",
    "    img_files_eval,\n",
    "    embds_folder_dino=embds_folder_geo,\n",
    "    embds_folder_sd=embds_folder_sd,\n",
    "    flips=flips_eval,\n",
    ")\n",
    "img_embds_eval = img_embds_eval.detach().cpu()\n",
    "img_embds_hat_eval = img_embds_hat_eval.detach().cpu()\n",
    "\n",
    "# build kpt_idx_to_kpt_embds\n",
    "kpt_idx_to_kpt_embds_train, kpt_embd_coords_train, kpt_img_coords_train = spair_utils.build_kpt_idx_to_kpt_embds(\n",
    "    img_files=img_files_train,\n",
    "    img_embds_hat=img_embds_hat_train,\n",
    "    spair_data_folder=spair_data_folder,\n",
    "    img_size=img_size,\n",
    "    embd_size=embd_size,\n",
    "    pad=pad,\n",
    "    flips=flips_train\n",
    ")\n",
    "kpt_idx_to_kpt_embds_eval, kpt_embd_coords_eval, kpt_img_coords_eval = spair_utils.build_kpt_idx_to_kpt_embds(\n",
    "    img_files=img_files_eval,\n",
    "    img_embds_hat=img_embds_hat_eval,\n",
    "    spair_data_folder=spair_data_folder,\n",
    "    img_size=img_size,\n",
    "    embd_size=embd_size,\n",
    "    pad=pad,\n",
    "    flips=flips_eval\n",
    ")\n",
    "\n",
    "# average keypoint embeddings\n",
    "kpt_features_avg_train = []\n",
    "kpt_features_attn_avg_train = []\n",
    "kpt_features_attn_sd_train = []\n",
    "for kpt_idx in range(30):\n",
    "    kpt_features = kpt_idx_to_kpt_embds_train[kpt_idx]\n",
    "    # kpt_features is only None if keypoint label does not exist for object category\n",
    "    if kpt_features is None:\n",
    "        break\n",
    "\n",
    "    if kpt_idx > 3 and kpt_idx < 22:\n",
    "        kpt_idx_2 = kpt_idx + 1 if kpt_idx % 2 == 0 else kpt_idx - 1\n",
    "        kpt_features_2 = kpt_idx_to_kpt_embds_train[kpt_idx_2]\n",
    "        assert kpt_features_2 is not None, f\"kpt_features is None for kpt_idx={kpt_idx_2}\"\n",
    "        kpt_features = torch.cat([kpt_features, kpt_features_2], dim=0)\n",
    "\n",
    "    kpt_features_avg = torch.mean(kpt_features, dim=0, keepdim=True)  # (1, C)\n",
    "    # compute dot product between kpt_features and kpt_features_avg\n",
    "    kpt_features_attn = torch.bmm(kpt_features.unsqueeze(0), kpt_features_avg.unsqueeze(2)).squeeze(0)\n",
    "    kpt_features_attn_avg = torch.mean(kpt_features_attn, dim=0, keepdim=True)\n",
    "    kpt_features_attn_sd = torch.std(kpt_features_attn, dim=0, keepdim=True)\n",
    "    kpt_features_avg_train.append(kpt_features_avg)\n",
    "    kpt_features_attn_avg_train.append(kpt_features_attn_avg)\n",
    "    kpt_features_attn_sd_train.append(kpt_features_attn_sd)\n",
    "\n",
    "kpt_features_avg_train = torch.cat(kpt_features_avg_train, dim=0)\n",
    "kpt_features_attn_avg_train = torch.cat(kpt_features_attn_avg_train, dim=0)\n",
    "kpt_features_attn_sd_train = torch.cat(kpt_features_attn_sd_train, dim=0)\n",
    "# print(kpt_features_avg_train.shape)\n",
    "\n",
    "# 1.1) load depth\n",
    "depths_train = []\n",
    "for img_file in img_files_train:\n",
    "    fn = os.path.basename(img_file).split(\".\")[0]\n",
    "    # depth_file = f\"{depth_folder}/{category}/depth_npy/{fn}_pred.npy\"\n",
    "    depth_file = f\"{depth_folder}/{category}/{fn}_depth.npy\"\n",
    "    depth = np.load(depth_file)\n",
    "    # NEW for depth any: normalize and adjust z\n",
    "    depth = 1 - 0.3*(depth/np.max(depth))\n",
    "\n",
    "    # NOTE instead of downsizing depth we will resize and upscale attentions\n",
    "    depths_train.append(depth)\n",
    "\n",
    "depths_eval = []\n",
    "for img_file in img_files_eval:\n",
    "    fn = os.path.basename(img_file).split(\".\")[0]\n",
    "    # depth_file = f\"{depth_folder}/{category}/depth_npy/{fn}_pred.npy\"\n",
    "    depth_file = f\"{depth_folder}/{category}/{fn}_depth.npy\"\n",
    "    depth = np.load(depth_file)\n",
    "    # NEW for depth any: normalize and adjust z\n",
    "    depth = 1 - 0.3*(depth/np.max(depth))\n",
    "    depths_eval.append(depth)\n",
    "\n",
    "# 1.3) reproject to point clouds\n",
    "xyz_train = []\n",
    "for depth in depths_train:\n",
    "    v0 = geom_utils.reproject_depth(depth, focal_length=5)\n",
    "    xyz_train.append(v0)\n",
    "\n",
    "xyz_eval = []\n",
    "for depth in depths_eval:\n",
    "    v0 = geom_utils.reproject_depth(depth, focal_length=5)\n",
    "    xyz_eval.append(v0)\n",
    "\n",
    "# load segmentation masks\n",
    "seg_masks_train = []\n",
    "seg_auto_masks_train = []\n",
    "for img_file in img_files_train:\n",
    "    fn = os.path.basename(img_file).split(\".\")[0]\n",
    "    seg_mask_file = f\"{masks_folder}/{category}/{fn}_masks.npy\"\n",
    "    seg_mask = np.load(seg_mask_file)\n",
    "    seg_masks_train.append(seg_mask)\n",
    "\n",
    "    seg_auto_mask_file = f\"{masks_folder}/{category}/{fn}_auto_masks_seg.npy\"\n",
    "    seg_auto_mask = np.load(seg_auto_mask_file)\n",
    "    seg_auto_masks_train.append(seg_auto_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 492.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1) compute input args for optimization procedure\n",
    "query_xy_all_normalized, depth_values_all, query_xyz_list, masks, kpt_coords_corrected_all = peak_extraction_utils.correct_all_keypoint_coords(\n",
    "    img_seg_masks=seg_masks_train,\n",
    "    kpt_img_coords=kpt_img_coords_train,\n",
    "    img_xyz_orig=xyz_train\n",
    ")\n",
    "\n",
    "img_shapes = torch.zeros((query_xy_all_normalized.shape[0], 2))\n",
    "for i in range(query_xy_all_normalized.shape[0]):\n",
    "    img_shapes[i,:] = torch.tensor(xyz_train[i].shape[:2])\n",
    "\n",
    "query_xy_all_normalized_no_nan = query_xy_all_normalized.clone()\n",
    "query_xy_all_normalized_no_nan[torch.isnan(query_xy_all_normalized_no_nan)] = 0.0\n",
    "depth_values_all_no_nan = depth_values_all.clone()\n",
    "depth_values_all_no_nan[torch.isnan(depth_values_all_no_nan)] = 0.0\n",
    "\n",
    "vertex_mask = focal_opt_utils.generate_is_not_nan_mask(query_xy_all_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 43.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(898.3763, grad_fn=<AddBackward0>)\n",
      "tensor([0.0695, 0.1217, 0.2202, 0.1692, 0.2185, 0.0827, 0.0806, 0.1130, 0.5963,\n",
      "        0.4924, 0.1507, 0.2364, 0.1747, 0.0535, 0.1560, 0.2952, 0.4853, 0.2327,\n",
      "        0.6674, 0.1343, 0.2510, 0.5226, 0.2163, 0.0749, 0.1647, 0.1250, 0.0913,\n",
      "        0.0656, 0.1009, 0.3605, 0.2692, 0.0761, 0.0658, 0.2662, 0.0279, 0.2007,\n",
      "        0.1049, 0.0831, 0.0727, 0.1322, 0.1372, 0.2687, 0.0507, 0.2723, 0.1514,\n",
      "        0.4628, 0.5780, 0.1523, 0.3653, 0.1992, 0.0309, 0.1183, 0.4052, 0.3786,\n",
      "        0.1808])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) run optimization loop\n",
    "focal_lengths_inv_opt_v2 = focal_opt_utils.optimize_focal_length_simple(\n",
    "    kpt_xy_normalized=query_xy_all_normalized_no_nan,\n",
    "    kpt_depth=depth_values_all_no_nan,\n",
    "    kpt_is_not_nan=vertex_mask,\n",
    "    img_shapes=img_shapes,\n",
    "    n_iter=1000,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# very similar results to the simple optimisation procedure\n",
    "# focal_lengths_inv_opt_v3 = focal_opt_utils.optimize_focal_length_global_local(\n",
    "#     kpt_xy_normalized=query_xy_all_normalized_no_nan,\n",
    "#     kpt_depth=depth_values_all_no_nan,\n",
    "#     kpt_is_not_nan=vertex_mask,\n",
    "#     img_shapes=img_shapes,\n",
    "#     n_global_iter=10,\n",
    "#     n_local_iter=100,\n",
    "#     lr=0.001\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics and visualize\n",
    "\n",
    "# reproject all points with optimised focal length\n",
    "kpts_xyz_all = focal_opt_utils.reproject_kpts(\n",
    "    focal_lenghts_inv=torch.abs(focal_lengths_inv_opt_v2),\n",
    "    img_shapes=img_shapes,\n",
    "    kpt_xy_normalized=query_xy_all_normalized_no_nan,\n",
    "    kpt_depth=depth_values_all_no_nan,\n",
    ")\n",
    "\n",
    "# compute statistics\n",
    "angles_mask, ratios_mask = focal_opt_utils.generate_angle_and_ratio_mask(vertex_mask)\n",
    "cos_angles, angles, ratios = kpt_likelihood_utils.compute_angles_and_ratios_parallel(kpts_xyz_all)\n",
    "cos_angles_mean, cos_angles_var, ratios_mean, ratios_var = kpt_likelihood_utils.compute_stats_over_images(\n",
    "    cos_angles, ratios, angles_mask, ratios_mask\n",
    ")\n",
    "\n",
    "# save results\n",
    "results = {\n",
    "    \"focal_lengths_inv_opt_v2\": focal_lengths_inv_opt_v2,\n",
    "    \"cos_angles_mean\": cos_angles_mean,\n",
    "    \"cos_angles_var\": cos_angles_var,\n",
    "    \"ratios_mean\": ratios_mean,\n",
    "    \"ratios_var\": ratios_var\n",
    "}\n",
    "out_dir = \"/home/user/Documents/projects/Metaworld/keyframes/align_ft_spair/notebooks\"\n",
    "for k, v in results.items():\n",
    "    np.save(f\"{out_dir}/focal_length_opt_{k}.npy\", torch_utils.to_np_array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 34\n",
    "print(focal_lengths_inv_opt_v2[img_idx])\n",
    "Image.open(img_files_train[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_xyz = geom_utils.reproject_depth(depths_train[img_idx], focal_length=1/(torch.abs(focal_lengths_inv_opt_v2[img_idx]).item()))\n",
    "img_xyz_flat = img_xyz.reshape((-1,3))\n",
    "kpt_labels = kpt_img_coords_train[img_idx][:,2].numpy()\n",
    "kpts_xyz = kpts_xyz_all[img_idx][kpt_labels]\n",
    "\n",
    "scatter = go.Scatter3d(\n",
    "    x=kpts_xyz[:,0],\n",
    "    y=kpts_xyz[:,1],\n",
    "    z=kpts_xyz[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=px.colors.qualitative.Alphabet,\n",
    "        # color=z,  # set color to an array/list of desired values\n",
    "        # colorscale='Viridis',  # choose a colorscale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    hovertext=[f\"{i}\" for i in kpt_labels]\n",
    ")\n",
    "\n",
    "scatter2 = go.Scatter3d(\n",
    "    x=img_xyz_flat[:,0],\n",
    "    y=img_xyz_flat[:,1],\n",
    "    z=img_xyz_flat[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=\"black\",\n",
    "        # color=z,  # set color to an array/list of desired values\n",
    "        # colorscale='Viridis',  # choose a colorscale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    # hovertext=[f\"{i}\" for i in kpt_labels]\n",
    ")\n",
    "\n",
    "fig = go.FigureWidget(data=[scatter2, scatter]) # scatter_img_xyz_orig\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0), scene_aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
